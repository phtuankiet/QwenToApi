from flask import Flask, request, Response, jsonify
import uuid
import time
import socket
import logging
import json

# Import cÃ¡c module Ä‘Ã£ tÃ¡ch
from utils.logging_config import setup_logging
from utils.queue_manager import queue_manager
from utils.terminal_ui import terminal_ui
from utils.chat_manager import chat_manager
from services.qwen_service import qwen_service
from services.chat_service import chat_service
from models.request_state import RequestState

# Cáº¥u hÃ¬nh Flask Ä‘á»ƒ tráº£ vá» JSON Ä‘áº¹p
app = Flask(__name__)
app.config['JSONIFY_PRETTYPRINT_REGULAR'] = True

# Cáº¥u hÃ¬nh Ä‘á»ƒ cháº¥p nháº­n header request ráº¥t dÃ i
app.config['MAX_CONTENT_LENGTH'] = 2048 * 1024 * 1024  # 2GB
app.config['MAX_CONTENT_PATH'] = None
app.config['MAX_COOKIE_SIZE'] = 2048 * 1024 * 1024  # 2GB

# TÄƒng giá»›i háº¡n cho request body
app.config['MAX_CONTENT_LENGTH'] = None  # KhÃ´ng giá»›i háº¡n

# Cáº¥u hÃ¬nh thÃªm Ä‘á»ƒ xá»­ lÃ½ request lá»›n
app.config['SEND_FILE_MAX_AGE_DEFAULT'] = 0
app.config['TEMPLATES_AUTO_RELOAD'] = True

# TÄƒng giá»›i háº¡n JSON serialization
import sys
sys.setrecursionlimit(10000)  # TÄƒng recursion limit

logger = setup_logging()

@app.route('/v1/models', methods=['GET'])
def list_models():
    """List the currently loaded models"""
    route_info = "GET /v1/models - List Models"
    logger.info(f"ROUTE: {route_info}")
    terminal_ui.update_route(route_info)
    
    models = qwen_service.get_models_from_qwen()
    response = jsonify({
        "object": "list",
        "data": models
    })
    response.headers['Content-Type'] = 'application/json'
    return response

@app.route('/v1/chat/completions', methods=['POST'])
def chat_completions():
    """Chat completions with streaming support"""
    data = request.get_json()
    stream = data.get('stream', False)
    model = data.get('model', 'qwen3-235b-a22b')
    
    route_info = f"POST /v1/chat/completions - Chat ({model}, stream: {stream})"
    logger.info(f"ROUTE: {route_info}")
    

    terminal_ui.update_route(route_info)
    
    if stream:
        logger.info("Starting streaming response...")
        return Response(
            stream_qwen_response_with_queue(data),
            mimetype='text/plain',
            headers={
                'Cache-Control': 'no-cache',
                'Connection': 'keep-alive',
                'Content-Type': 'text/event-stream'
            }
        )
    else:
        logger.info("Starting non-streaming response...")
        # Non-streaming response - proxy to Qwen API
        return stream_qwen_response_non_streaming_with_queue(data)

def stream_qwen_response_with_queue(data):
    """Stream response from Qwen API with queue system"""
    model = data.get('model', 'qwen3-235b-a22b')
    request_id = str(uuid.uuid4())
    
    logger.info(f"Stream function called with model: {model}, request_id: {request_id}")
    
    # Äá»£i cho Ä‘áº¿n khi cÃ³ thá»ƒ xá»­ lÃ½ vá»›i timeout
    if not queue_manager.acquire_lock(request_id):
        yield f"data: {json.dumps({'error': 'Server busy, request timed out'})}\n\n"
        return
    
    try:
        # Táº¡o request state cho request nÃ y
        request_state = RequestState(request_id, model)
        
        # Xá»­ lÃ½ request hiá»‡n táº¡i
        for chunk in chat_service.stream_qwen_response(data, request_state):
            yield chunk
    
    except Exception as e:
        logger.error(f"Error in stream_qwen_response_with_queue: {e}")
        yield f"data: {json.dumps({'error': f'Stream error: {str(e)}'})}\n\n"
    finally:
        # Äáº£m báº£o lock Ä‘Æ°á»£c release
        queue_manager.release_lock(request_id)

def stream_qwen_response_non_streaming_with_queue(data):
    """Non-streaming response from Qwen API with queue system"""
    model = data.get('model', 'qwen3-235b-a22b')
    request_id = str(uuid.uuid4())
    
    logger.info(f"Non-streaming function called with model: {model}, request_id: {request_id}")
    
    # Kiá»ƒm tra náº¿u cÃ³ request Ä‘ang xá»­ lÃ½
    with queue_manager.chat_lock:
        if queue_manager.current_processing:
            logger.info(f"Non-streaming request {request_id} waiting in queue. Queue size: {len(queue_manager.chat_queue)}")
            return jsonify({
                "error": {
                    "message": "Server busy, please try again later",
                    "type": "server_error",
                    "code": "server_busy"
                }
            }), 503
        else:
            queue_manager.current_processing = True
            queue_manager.current_processing_start_time = time.time()
            logger.info(f"Non-streaming request {request_id} starting processing")
    
    try:
        # Xá»­ lÃ½ request hiá»‡n táº¡i
        result = chat_service.stream_qwen_response_non_streaming(data)
        
        # Xá»­ lÃ½ queue sau khi hoÃ n thÃ nh - Ä‘Ã£ Ä‘Æ°á»£c xá»­ lÃ½ trong finally block
        
        return result
    
    except Exception as e:
        logger.error(f"Error in stream_qwen_response_non_streaming_with_queue: {e}")
        return jsonify({
            "error": {
                "message": f"Stream error: {str(e)}",
                "type": "server_error"
            }
        }), 500
    finally:
        # Äáº£m báº£o lock Ä‘Æ°á»£c release
        queue_manager.release_lock(request_id)

@app.errorhandler(500)
def internal_error(error):
    """Handle internal server errors and release lock"""
    route_info = "ERROR 500 - Internal Server Error"
    logger.error(f"ROUTE: {route_info}")
    terminal_ui.update_route(route_info)
    
    logger.error(f"Internal server error: {error}")
    queue_manager.release_lock("error_handler")
    return jsonify({
        "error": {
            "message": "Internal server error",
            "type": "server_error"
        }
    }), 500

@app.route('/v1/completions', methods=['POST'])
def completions():
    """Text completions (deprecated)"""
    route_info = "POST /v1/completions - Deprecated"
    logger.info(f"ROUTE: {route_info}")
    terminal_ui.update_route(route_info)
    
    return jsonify({
        "error": {
            "message": "This endpoint is deprecated. Use /v1/chat/completions instead.",
            "type": "invalid_request_error",
            "code": "deprecated_endpoint"
        }
    }), 400

@app.route('/v1/embeddings', methods=['POST'])
def embeddings():
    """Text embeddings"""
    route_info = "POST /v1/embeddings - Not Supported"
    logger.info(f"ROUTE: {route_info}")
    terminal_ui.update_route(route_info)
    
    return jsonify({
        "error": {
            "message": "Embeddings not supported in this server",
            "type": "invalid_request_error",
            "code": "not_supported"
        }
    }), 400

@app.route('/v1/test/large_request', methods=['POST'])
def test_large_request():
    """Test endpoint Ä‘á»ƒ kiá»ƒm tra request lá»›n"""
    route_info = "POST /v1/test/large_request - Test Large Request"
    logger.info(f"ROUTE: {route_info}")
    terminal_ui.update_route(route_info)
    
    try:
        # Äá»c raw data
        raw_data = request.get_data()
        logger.info(f"Test large request - Raw data length: {len(raw_data)} bytes")
        
        # Parse JSON
        data = request.get_json()
        
        # Táº¡o response vá»›i thÃ´ng tin chi tiáº¿t
        response_info = {
            "success": True,
            "raw_data_length": len(raw_data),
            "parsed_data_keys": list(data.keys()) if data else [],
            "content_length": request.content_length,
            "messages_count": len(data.get('messages', [])) if data else 0,
            "server_config": {
                "max_content_length": app.config.get('MAX_CONTENT_LENGTH'),
                "max_cookie_size": app.config.get('MAX_COOKIE_SIZE')
            }
        }
        
        # LÆ°u test data
        with open("test_large_request.txt", "w", encoding='utf-8') as f:
            f.write(raw_data.decode('utf-8', errors='ignore'))
        
        return jsonify(response_info)
        
    except Exception as e:
        logger.error(f"Test large request error: {e}")
        return jsonify({"error": str(e)}), 500

if __name__ == '__main__':
    # Láº¥y IP address
    hostname = socket.gethostname()
    local_ip = socket.gethostbyname(hostname)
    
    # Startup logs
    logger.info("Success! HTTP server listening on port 1235")
    logger.warning("Server accepting connections from the local network. Only use this if you know what you are doing!")
    logger.info("")
    logger.info("Supported endpoints:")
    logger.info(f"->\tGET  http://{local_ip}:1235/v1/models")
    logger.info(f"->\tPOST http://{local_ip}:1235/v1/chat/completions")
    logger.info(f"->\tPOST http://{local_ip}:1235/v1/completions")
    logger.info(f"->\tPOST http://{local_ip}:1235/v1/embeddings")
    logger.info("")
    logger.info("Custom server with Qwen API integration")
    logger.info("Queue system enabled - requests will be queued if server is busy")
    logger.info("Think mode support enabled - <think> and </think> tags for Qwen thinking phase")
    logger.info("Lock timeout: 2 minutes, Request timeout: 60 seconds")
    logger.info("Server started.")
    logger.info("Just-in-time model loading active.")
    
    # Khá»Ÿi táº¡o chat_id khi server báº¯t Ä‘áº§u
    logger.info("Initializing chat session...")
    chat_id = chat_manager.initialize_chat()
    if chat_id:
        logger.info(f"Chat initialized with ID: {chat_id}")
        terminal_ui.update_chat_id(chat_id)
        terminal_ui.update_parent_id(None)  # Reset parent_id khi khá»Ÿi táº¡o
    else:
        logger.error("Failed to initialize chat")
    
    # Báº¯t Ä‘áº§u terminal UI
    terminal_ui.start()
    
    try:
        # Cáº¥u hÃ¬nh Werkzeug Ä‘á»ƒ cháº¥p nháº­n header ráº¥t dÃ i
        from werkzeug.serving import WSGIRequestHandler
        WSGIRequestHandler.max_requestline = 2048 * 1024 * 1024  # 2GB
        WSGIRequestHandler.max_header_size = 2048 * 1024 * 1024  # 2GB
        
        # Cáº¥u hÃ¬nh thÃªm cho Werkzeug
        import werkzeug
        werkzeug.serving.WSGIRequestHandler.max_requestline = 2048 * 1024 * 1024
        werkzeug.serving.WSGIRequestHandler.max_header_size = 2048 * 1024 * 1024
        
        # Táº¯t giá»›i háº¡n request size
        werkzeug.serving.WSGIRequestHandler.max_requestline = None
        werkzeug.serving.WSGIRequestHandler.max_header_size = None
        
        # TÄƒng buffer size cho socket
        import socket
        socket.SOMAXCONN = 1024
        
        # TÄƒng buffer size cho request
        import os
        os.environ['FLASK_MAX_CONTENT_LENGTH'] = str(2048 * 1024 * 1024)  # 2GB
        os.environ['FLASK_MAX_CONTENT_LENGTH'] = '0'  # KhÃ´ng giá»›i háº¡n
        
        app.run(host='0.0.0.0', port=1235, debug=False, threaded=True, processes=1)
    except KeyboardInterrupt:
        print("\nðŸ›‘ Server stopped by user")
    finally:
        terminal_ui.stop()
