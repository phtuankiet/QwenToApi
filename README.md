# QwenToApi v1.0.1 - Qwen API Integration

Server tÃ¹y chá»‰nh tÃ­ch há»£p vá»›i Qwen API, há»— trá»£ cáº£ LM Studio vÃ  Ollama format vá»›i giao diá»‡n GUI hiá»‡n Ä‘áº¡i.

## ğŸš€ TÃ­nh nÄƒng

- **Dual Mode**: Há»— trá»£ cáº£ LM Studio (port 1235) vÃ  Ollama (port 11434)
- **Modern GUI**: Giao diá»‡n ngÆ°á»i dÃ¹ng hiá»‡n Ä‘áº¡i vá»›i responsive design
- **Think Mode**: Há»— trá»£ tÃ­nh nÄƒng suy nghÄ© cá»§a Qwen vá»›i `<think>` tags
- **Image Support**: Há»— trá»£ xá»­ lÃ½ hÃ¬nh áº£nh base64 (Ollama mode)
- **Queue System**: Há»‡ thá»‘ng xáº¿p hÃ ng Ä‘á»ƒ xá»­ lÃ½ request Ä‘á»“ng thá»i
- **Background Mode**: Cháº¡y server trong background khÃ´ng cÃ³ output
- **Real-time Monitoring**: Theo dÃµi tráº¡ng thÃ¡i server vÃ  queue real-time
- **Model Management**: Hiá»ƒn thá»‹ vÃ  quáº£n lÃ½ models vá»›i copy functionality
- **Chat Management**: Quáº£n lÃ½ chat sessions vá»›i chat ID tracking

## ğŸ“‹ YÃªu cáº§u há»‡ thá»‘ng

- Python 3.7+
- Windows 10/11 (GUI mode)
- Internet connection Ä‘á»ƒ truy cáº­p Qwen API

## ğŸ› ï¸ CÃ i Ä‘áº·t

```bash
# Clone repository
git clone https://github.com/khanhnguyen9872/custom_server_lmstudio.git
cd custom_server_lmstudio

# CÃ i Ä‘áº·t dependencies
pip install -r requirements.txt
```

## ğŸ® Sá»­ dá»¥ng

### GUI Mode (Khuyáº¿n nghá»‹)
```bash
python main.py
```

### Terminal Mode
```bash
python server.py
```

### Background Mode
```bash
python server.py --background
```

### Command Line Arguments

```bash
# Cháº¡y trong background mode (khÃ´ng cÃ³ output)
python server.py --background

# Chá»‰ Ä‘á»‹nh mode
python server.py --mode lmstudio
python server.py --mode ollama

# Chá»‰ Ä‘á»‹nh port
python server.py --port 1235
python server.py --port 11434

# Chá»‰ Ä‘á»‹nh host
python server.py --host 127.0.0.1

# Káº¿t há»£p nhiá»u argument
python server.py --background --mode ollama --port 11434
```

## ğŸ–¥ï¸ GUI Features

### Dashboard Tab
- **Server Status**: Hiá»ƒn thá»‹ tráº¡ng thÃ¡i server real-time
- **Current Route**: Theo dÃµi route vÃ  request body hiá»‡n táº¡i
- **Queue Status**: Hiá»ƒn thá»‹ tráº¡ng thÃ¡i queue vÃ  thá»i gian xá»­ lÃ½
- **Chat Controls**: Quáº£n lÃ½ chat sessions

### Logs Tab
- **Real-time Logs**: Hiá»ƒn thá»‹ logs server theo thá»i gian thá»±c
- **Log Management**: Clear logs vÃ  filter

### Settings Tab
- **Server Configuration**: Cáº¥u hÃ¬nh IP, port, mode
- **UI Scale**: Äiá»u chá»‰nh kÃ­ch thÆ°á»›c giao diá»‡n (100% - 200%)
- **Cookie Management**: Quáº£n lÃ½ Qwen cookie
- **Model Cache**: Refresh model cache

### Keyboard Shortcuts
- `Ctrl+S` - Start Server
- `Ctrl+Q` - Stop Server
- `Ctrl+N` - New Chat
- `Ctrl+M` - Show Models
- `Ctrl+R` - Show Routes
- `F1` - Show Help
- `F2` - Show About
- `F5` - Refresh Status
- `Escape` - Close Popups

## ğŸ”Œ API Endpoints

### LM Studio Mode (port 1235)

- `GET /` - Root endpoint
- `GET /v1/models` - Danh sÃ¡ch models
- `GET /v1/models/{model_id}` - ThÃ´ng tin model
- `POST /v1/chat/completions` - Chat completions
- `POST /v1/completions` - Text completions (deprecated)
- `POST /v1/embeddings` - Embeddings (not supported)

### Ollama Mode (port 11434)

- `GET /` - Root endpoint
- `GET /api/version` - PhiÃªn báº£n Ollama
- `GET /api/tags` - Danh sÃ¡ch models
- `GET /api/ps` - Models Ä‘ang cháº¡y
- `POST /api/show` - ThÃ´ng tin model
- `POST /api/generate` - Generate response
- `POST /api/chat` - Chat endpoint
- `GET|POST /v1/*` - With All API of LM Studio

## ğŸ§  Think Mode

### LM Studio
```json
{
  "model": "qwen3-235b-a22b",
  "messages": [{"role": "user", "content": "HÃ£y suy nghÄ© vá» cÃ¢u há»i nÃ y"}],
  "stream": true
}
```

Response sáº½ cÃ³ `<think>` tags:
```
<think>Äang suy nghÄ© vá» cÃ¢u há»i...</think>
CÃ¢u tráº£ lá»i cá»§a tÃ´i lÃ ...
```

### Ollama
```json
{
  "model": "qwen3-235b-a22b",
  "messages": [{"role": "user", "content": "HÃ£y suy nghÄ© vá» cÃ¢u há»i nÃ y"}],
  "stream": true
}
```

Response sáº½ cÃ³ `thinking` field:
```json
{
  "model": "qwen3-235b-a22b",
  "message": {
    "role": "assistant",
    "thinking": "Äang suy nghÄ© vá» cÃ¢u há»i...",
    "content": "CÃ¢u tráº£ lá»i cá»§a tÃ´i lÃ ..."
  }
}
```

## ğŸ“Š Queue System

Server sá»­ dá»¥ng há»‡ thá»‘ng xáº¿p hÃ ng Ä‘á»ƒ xá»­ lÃ½ request Ä‘á»“ng thá»i:
- **Timeout**: 2 phÃºt
- **Request timeout**: 60 giÃ¢y
- **Concurrent processing**: Chá»‰ xá»­ lÃ½ 1 request táº¡i má»™t thá»i Ä‘iá»ƒm
- **Real-time monitoring**: Hiá»ƒn thá»‹ tráº¡ng thÃ¡i queue trong GUI

## ğŸ“ Logs

- **File logs**: ÄÆ°á»£c lÆ°u trong thÆ° má»¥c `logs/` vá»›i format ngÃ y thÃ¡ng
- **GUI logs**: Hiá»ƒn thá»‹ real-time trong tab Logs
- **Log rotation**: Tá»± Ä‘á»™ng quáº£n lÃ½ kÃ­ch thÆ°á»›c logs

## ğŸ”§ Configuration

### Settings File
CÃ¡c cÃ i Ä‘áº·t Ä‘Æ°á»£c lÆ°u trong `ui_settings.json`:
- UI Scale
- IP Address
- Port
- Server Mode
- Selected Model
- Cookie Value

### Cookie Setup
1. Má»Ÿ Qwen chat trong browser
2. Copy cookie tá»« Developer Tools
3. Paste vÃ o Settings tab trong GUI
4. Save configuration

## ğŸš¨ Troubleshooting

### Server khÃ´ng khá»Ÿi Ä‘á»™ng
- Kiá»ƒm tra port cÃ³ Ä‘ang Ä‘Æ°á»£c sá»­ dá»¥ng khÃ´ng
- Kiá»ƒm tra cookie trong Settings
- Kiá»ƒm tra káº¿t ná»‘i internet
- Cháº¡y vá»›i quyá»n administrator náº¿u cáº§n

### Request bá»‹ timeout
- Server Ä‘ang xá»­ lÃ½ request khÃ¡c
- Äá»£i 2 phÃºt hoáº·c restart server
- Kiá»ƒm tra tráº¡ng thÃ¡i queue trong GUI

### Think mode khÃ´ng hoáº¡t Ä‘á»™ng
- Äáº£m báº£o model há»— trá»£ think mode
- Kiá»ƒm tra response format
- Thá»­ vá»›i model khÃ¡c

### GUI khÃ´ng hiá»ƒn thá»‹
- Kiá»ƒm tra Python version (3.7+)
- CÃ i Ä‘áº·t tkinter: `pip install tk`
- Cháº¡y trÃªn Windows 10/11

## ğŸ“¦ Build

### Windows Executable
```bash
# Sá»­ dá»¥ng Nuitka
python -m nuitka --onefile --windows-icon-from-ico=qwen.ico main.py

# Hoáº·c sá»­ dá»¥ng PyInstaller
pyinstaller --onefile --windowed --icon=qwen.ico main.py
```

## ğŸ¤ Contributing

1. Fork repository
2. Táº¡o feature branch
3. Commit changes
4. Push to branch
5. Táº¡o Pull Request

## ğŸ“„ License

MIT License - Xem file [LICENSE](LICENSE) Ä‘á»ƒ biáº¿t thÃªm chi tiáº¿t.

## ğŸ‘¨â€ğŸ’» Developer

**KhanhNguyen9872**

- GitHub: [@khanhnguyen9872](https://github.com/khanhnguyen9872)
- Repository: [custom_server_lmstudio](https://github.com/khanhnguyen9872/custom_server_lmstudio)

## ğŸ”„ Changelog

### v1.0.1
- âœ¨ ThÃªm GUI hiá»‡n Ä‘áº¡i vá»›i responsive design
- ğŸ¯ Hiá»ƒn thá»‹ phiÃªn báº£n trong UI
- ğŸ“Š Real-time monitoring dashboard
- ğŸ® Keyboard shortcuts
- ğŸ”§ Settings management
- ğŸ“ Log viewer
- ğŸ§  Chat management
- ğŸ¨ UI scaling (100% - 200%)
- ğŸ“± Responsive layout
- ğŸ”„ Model cache refresh

### v1.0.0
- ğŸš€ Initial release
- ğŸ”Œ Dual mode support (LM Studio & Ollama)
- ğŸ§  Think mode implementation
- ğŸ–¼ï¸ Image support
- ğŸ“Š Queue system
- ğŸ”§ Background mode
